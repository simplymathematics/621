---
title: "HW 4"
author: "Team 2"
date: "April 24, 2019"
output:
  pdf_document:
    toc: yes
  html_document:
    pdf_document: default
    theme: cosmo
    toc: yes
    toc_float: yes
---

# OVERVIEW

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero

## Dependencies

Replication of our work requires the following packages in Rstudio:

```{r, echo=F, message=F, warning=F, error=F, comment=F}
# Set working directory
try(setwd("~/GitHub/621/HW4"))

# Requirements for formatting and augmenting default settings for chunks. 
library(knitr)
library(kableExtra)
library(default)

knitr::opts_chunk$set(echo=F, message=F, warning=F, error=F, comment=F) 

default(kable_styling)  <- list(bootstrap_options = c("basic"), 
                                position = "center", 
                                full_width = T,
                                font_size = NULL)

default(row_spec)  <- list(row = 0:0, bold = T)
```

```{r, echo=T}
library(ggplot2)
library(ggpubr)
library(dplyr)
library(tidyr)
library(corrplot)
library(randomForest)
library(olsrr)
library(psych)
```

## Objective 

Our objective is to build multiple linear regression and binary logistic regression models on the `training` data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car.

# PART 1: DATA EXPLORATION

First, we read the data as a csv and then examined the below variable from the `training` dataset. 

```{r}
training <- as.data.frame(read.csv("insurance_training_data.csv"))
test <- as.data.frame(read.csv("insurance-evaluation-data.csv"))

include_graphics("./hw4_vars.jpg")
```



## Summary Statistics 


We look at summary of the data;
```{r, echo = FALSE}

training<-training[,-1]

summary(training)%>% kable() %>% kable_styling()

```


We can see that some variables have missing values, so we will have to impute them later. For home and income values, we imputed them as the mean, which doesn't change the variance. However, this assumes that these people have both income and homes. If not, this would bias our models. We must also convert the currency data to a numeric form. One record has a car age of -3, which was also replaced with the mean. There are 4 records where Job is blank, which will be treated as its own category.


```{r, echo = FALSE}

training$TARGET_FLAG<-as.factor(training$TARGET_FLAG)

training$HOME_VAL<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$HOME_VAL)))))

training$INCOME<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$INCOME)))))

training$BLUEBOOK<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$BLUEBOOK)))))

training$OLDCLAIM<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$OLDCLAIM)))))

for(i in c(4,6,7,9,24)){
  training[is.na(training[,i]), i] <- mean(training[,i], na.rm = TRUE)
}


training$CAR_AGE[training$CAR_AGE<0] <- mean(training$CAR_AGE)

#training$JOB[training$JOB=='']

training$CAR_AGE[training$CAR_AGE<0]

training$JOB[training$JOB=='']

```

## Density


Below, we examine the distribution of values for each of the variables.

```{r,echo = FALSE, fig.width=8, fig.height=6}

training%>%
  select_if(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density() 
```


Age and YOJ appear to be fairly normally distributed. However, Claim frequency, home value, home kids, kids driving, and mvr pts appear to follow either continuous or discrete quasi-Poisson processes. In any case, claim frequency, homekids, kidsdriv, mvr_pts, oldclaim, tif, travtime, and yoj are count values. Log transforms will be inappropriate and skew the data, particularly the ones with 

```{r,echo = FALSE, fig.width=5, fig.height=3}

ggplot(training, aes(x=TARGET_FLAG, y=log(TARGET_AMT+1))) + 
  geom_violin()


ggplot(training, aes(x=MSTATUS, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=SEX, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=EDUCATION, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=CAR_USE, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=CAR_TYPE, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=RED_CAR, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=REVOKED, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=URBANICITY, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=JOB, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=PARENT1, y=log(TARGET_AMT+1))) + 
  geom_violin()
```

## Scatter plot matrix

We then build scatter plot matrix for continious variables

```{r,, echo = FALSE, fig.width=12, fig.height=8,warning=FALSE,message=FALSE}
require(psych)


training%>%select_if(is.numeric)%>%pairs.panels( 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE  # show density plots
             )

```

## Correlation  

We can see our correlation matrix below. A dark blue circle represents a strong positive relationship and a dark red circle represents a strong negative relationship between two variables. 
```{r, echo = FALSE}
results1 <- training%>%
 select_if(is.numeric) %>% cor(method = 'pearson', use = 'complete.obs')
corrplot::corrplot(results1, method = 'circle')


```

Finally, we can use the `randomforest` package to verify our assumptions from the correlation plot.

```{r, echo = FALSE,fig.width=8, fig.height=6}
training2 <- training
training2$TARGET_AMT<- NULL
training2$TARGET_FLAG<- NULL
target1 <- training$TARGET_AMT
target2 <- training$TARGET_FLAG

fit1 <- randomForest(training2, target1, importance = TRUE, ntree = 50)

varImpPlot(fit1)

fit2 <- randomForest(training2, target2, importance = TRUE, ntree = 50)

varImpPlot(fit2)

```


# Data Preparation

In the following section we will prepare and transform our variables for our model. For incomplete cases, we replaced the value of NULL data with the mean of the relevant data vector.

```{r, echo = FALSE}
training$AGE[is.na(training$AGE)] <- mean(training$AGE, na.rm=TRUE)

training$YOJ[is.na(training$YOJ)] <- mean(training$YOJ, na.rm=TRUE)

training$HOME_VAL[is.na(training$HOME_VAL)] <- mean(training$HOME_VAL, na.rm=TRUE)

training$CAR_AGE[is.na(training$CAR_AGE)] <- mean(training$CAR_AGE, na.rm=TRUE)

training$INCOME[is.na(training$INCOME)] <- mean(training$INCOME, na.rm=TRUE)
```

We must also convert all of the education categories into numerical ones.

```{r}
training <- data.frame(lapply(training, function(x){
  gsub("z_", "", x)
  
}))
training <- data.frame(lapply(training, function(x){
  gsub("<High School", "High School", x)
  
}))
training <- data.frame(lapply(training, function(x){
  gsub("z_", "", x)
  
}))
```




