---
title: "HW 4"
author: "Team 2"
date: "April 12, 2019"
output:
  html_document:
    pdf_document: default
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

# Overview

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000
records representing a customer at an auto insurance company. Each record has two response variables. The
first response variable, TARGET_FLAG, is a 1 or a 0. A "1" means that the person was in a car crash. A zero
means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero
if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero

List of variables:

|VARIABLE_NAME  |DEFINITION                                         |THEORETICAL_EFFECT                                                                   |
|---------------|---------------------------------------------------|-------------------------------------------------------------------------------------|
|INDEX         |Identification Variable (do not use)                |None                                                                                 |
|TARGET_FLAG   |Was Car in a crash? 1=YES 0=NO                      |None                                                                                 |
|TARGET_AMT    |If car was in a crash, what was the cost            |None                                                                                 |
|AGE           |Age of Driver                                       |Very young people tend to be risky. Maybe very old people also.                      |
|BLUEBOOK      |Value of Vehicle                                    |Unknown effect on probability of collision, but probably effect the payout if there i| |              |                                                    | crash                                                                               |
|CAR_AGE       |Vehicle Age                                         |Unknown effect on probability of collision, but probably effect the payout if there i| |              |                                                    |  crash                                                                              |
|CAR_TYPE      |Type of Car                                         |Unknown effect on probability of collision, but probably effect the payout if there i| |              |                                                    |crash                                                                                |
|CAR_USE       |Vehicle Use                                         |Commercial vehicles are driven more, so might increase probability of collision      |
|CLM_FREQ      |# Claims (Past 5 Years)                             |The more claims you filed in the past, the more you are likely to file in the future |
|EDUCATION     |Max Education Level                                 |Unknown effect, but in theory more educated people tend to drive more safely         |
|HOMEKIDS      |# Children at Home                                  |Unknown effect                                                                       |
|HOME_VAL      |Home Value                                          |In theory, home owners tend to drive more responsibly                                |
|INCOME        |Income                                              |In theory, rich people tend to get into fewer crashes                                |
|JOB           |Job Category                                        |In theory, white collar jobs tend to be safer                                        |
|KIDSDRIV      |# Driving Children                                  |When teenagers drive your car, you are more likely to get into crashes               |
|MSTATUS       |Marital Status                                      |In theory, married people drive more safely                                          |
|MVR_PTS       |Motor Vehicle Record Points                         |If you get lots of traffic tickets, you tend to get into more crashes                |
|OLDCLAIM      |Total Claims (Past 5 Years)                         |If your total payout over the past five years was high, this suggests future payouts | |              |                                                    | will be high                                                                        |
|PARENT1       |Single Parent                                       |Unknown effect                                                                       |
|RED_CAR       |A Red Car                                           |Urban legend says that red cars (especially red sports cars) are more risky. Is that  | |              |                                                    |   true?                                                                              |
|REVOKED       |License Revoked (Past 7 Years)                      |If your license was revoked in the past 7 years, you probably are a more risky driver.|
|SEX           |Gender                                              |Urban legend says that women have less crashes then men. Is that true?                |
|TIF           |Time in Force                                       |People who have been customers for a long time are usually more safe.                 |
|TRAVTIME      |Distance to Work                                    |Long drives to work usually suggest greater risk                                      |
|URBANICITY    |Home/Work Area                                      |Unknown                                                                               |
|YOJ           |Years on Job                                        |People who stay at a job for a long time are usually more safe                        |

## Objective 

Our objective is to build multiple linear regression and binary logistic regression models on the training data
to predict the probability that a person will crash their car and also the amount of money it will cost if the person
does crash their car.

## Dependencies

Replication of our work requires the following packages in Rstudio:

```{r, echo = TRUE, message=FALSE, warning=FALSE, error=FALSE, comment=FALSE}
#install.packages('corrplot')
#install.packages('randomForest')  

require(ggplot2)
require(dplyr)
require(tidyr)
require(corrplot)
require(randomForest)
require(olsrr)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, comment=FALSE}
# Requirements for formatting and augmenting default settings for chunks. 
require(knitr)
require(kableExtra)
require(default)

knitr::opts_chunk$set(echo = T, message = T, warning = T) # change message/warning to F when finished

default(kable_styling)  <- list(bootstrap_options = c("basic"), 
                                position = "center", 
                                full_width = TRUE,
                                font_size = NULL)
```

# Data Exploration

First, we read the data as a csv then performed some simple statistical calculations so that we could explore the data. Below we can see a sample of the data output as it was read from the csv.

```{r, echo = FALSE}
training <- as.data.frame(read.csv("insurance_training_data.csv"))
test <- as.data.frame(read.csv("insurance-evaluation-data.csv"))

training %>% slice(1:5) %>% kable() %>% kable_styling()
```


## Summary Statistics 

We look at summary of the data;
```{r, echo = FALSE}

training<-training[,-1]

summary(training)%>% kable() %>% kable_styling()

```

We can see that some variables have missing values, so we will have to impute them later. For home and income values, we imputed them as the mean, which doesn't change the variance. However, this assumes that these people have both income and homes. If not, this would bias our models. We must also convert the currency data to a numeric form. One record has a car age of -3, which was also replaced with the mean. There are 4 records where Job is blank, which will be treated as its own category.

```{r, echo = FALSE}

training$TARGET_FLAG<-as.factor(training$TARGET_FLAG)

training$HOME_VAL<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$HOME_VAL)))))

training$INCOME<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$INCOME)))))

training$BLUEBOOK<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$BLUEBOOK)))))

training$OLDCLAIM<-as.numeric(gsub(",","",(gsub("\\$","",as.character(training$OLDCLAIM)))))

for(i in c(4,6,7,9,24)){
  training[is.na(training[,i]), i] <- mean(training[,i], na.rm = TRUE)
}

training$CAR_AGE[training$CAR_AGE<0] <- mean(training$CAR_AGE)

#training$JOB[training$JOB=='']
```

## Density

Below, we examine the distribution of values for each of the variables.
```{r,echo = FALSE, fig.width=8, fig.height=6}

training%>%
  select_if(is.numeric) %>%                     # Keep only numeric columns
  gather() %>%                             # Convert to key-value pairs
  ggplot(aes(value)) +                     # Plot the values
    facet_wrap(~ key, scales = "free") +   # In separate panels
    geom_density() 
```

Age and YOJ appear to be fairly normally distributed. However, Claim frequency, home value, home kids, kids driving, and mvr pts appear to follow either continuous or discrete quasi-Poisson processes. In any case, claim frequency, homekids, kidsdriv, mvr_pts, oldclaim, tif, travtime, and yoj are count values. Log transforms will be inappropriate and skew the data, particularly the ones with 

```{r,echo = FALSE, fig.width=5, fig.height=3}

ggplot(training, aes(x=TARGET_FLAG, y=log(TARGET_AMT+1))) + 
  geom_violin()


ggplot(training, aes(x=MSTATUS, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=SEX, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=EDUCATION, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=CAR_USE, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=CAR_TYPE, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=RED_CAR, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=REVOKED, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=URBANICITY, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=JOB, y=log(TARGET_AMT+1))) + 
  geom_violin()

ggplot(training, aes(x=PARENT1, y=log(TARGET_AMT+1))) + 
  geom_violin()
```

## Scatter plot matrix

We then build scatter plot matrix for continious variables

```{r,, echo = FALSE, fig.width=12, fig.height=8,warning=FALSE,message=FALSE}
require(psych)


training%>%select_if(is.numeric)%>%pairs.panels( 
             method = "pearson", # correlation method
             hist.col = "#00AFBB",
             density = TRUE  # show density plots
             )

```

## Correlation  

We can see our correlation matrix below. A dark blue circle represents a strong positive relationship and a dark red circle represents a strong negative relationship between two variables. 
```{r, echo = FALSE}
results1 <- training%>%
 select_if(is.numeric) %>% cor(method = 'pearson', use = 'complete.obs')
corrplot::corrplot(results1, method = 'circle')


```

Finally, we can use the `randomforest` package to verify our assumptions from the correlation plot.

```{r, echo = FALSE,fig.width=8, fig.height=6}
training2 <- training
training2$TARGET_AMT<- NULL
training2$TARGET_FLAG<- NULL
target1 <- training$TARGET_AMT
target2 <- training$TARGET_FLAG

fit1 <- randomForest(training2, target1, importance = TRUE, ntree = 50)

varImpPlot(fit1)

fit2 <- randomForest(training2, target2, importance = TRUE, ntree = 50)

varImpPlot(fit2)

```


# Data Preparation
In the following section we will prepare and transform our variables for our model. For incomplete cases, we replaced the value of NULL data with the mean of the relevant data vector.

```{r, echo = FALSE}
training$AGE[is.na(training$AGE)] <- mean(training$AGE, na.rm=TRUE)

training$YOJ[is.na(training$YOJ)] <- mean(training$YOJ, na.rm=TRUE)

training$HOME_VAL[is.na(training$HOME_VAL)] <- mean(training$HOME_VAL, na.rm=TRUE)

training$CAR_AGE[is.na(training$CAR_AGE)] <- mean(training$CAR_AGE, na.rm=TRUE)

training$INCOME[is.na(training$INCOME)] <- mean(training$INCOME, na.rm=TRUE)
```

We must also convert all of the education categories into numerical ones.

```{r}
training <- data.frame(lapply(training, function(x){
  gsub("z_", "", x)
  
}))
training <- data.frame(lapply(training, function(x){
  gsub("<High School", "High School", x)
  
}))
training <- data.frame(lapply(training, function(x){
  gsub("z_", "", x)
  
}))

training <- data.frame(lapply(training, function(x){
  gsub("PhD", "4", x)
  
}))
training <- data.frame(lapply(training, function(x){
  gsub("Masters", "3", x)
  
}))
training <- data.frame(lapply(training, function(x){
  gsub("Bachelors", "2", x)
  
}))
training <- data.frame(lapply(training, function(x){
  gsub("High School", "1", x)
  
}))
unique(training$EDUCATION)
```








