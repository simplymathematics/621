---
title: "HW 3"
author: "Team 2"
date: "April 10, 2019"
output:
  pdf_document:
    toc: yes
  html_document:
    pdf_document: default
    theme: cosmo
    toc: yes
    toc_float: yes
---

# Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). Below is a short description of the variables of interest in the data set: 
 
1. `zn`: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
2. `indus`: proportion of non-retail business acres per suburb (predictor variable)
3. `chas`: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
4. `nox`: nitrogen oxides concentration (parts per 10 million) (predictor variable)
5. `rm`: average number of rooms per dwelling (predictor variable)
6. `age`: proportion of owner-occupied units built prior to 1940 (predictor variable)
7. `dis`: weighted mean of distances to five Boston employment centers (predictor variable)
8. `rad`: index of accessibility to radial highways (predictor variable)
9. `tax`: full-value property-tax rate per $10,000 (predictor variable)
10. `ptratio`: pupil-teacher ratio by town (predictor variable)
11. `black`: 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town (predictor variable)
12. `lstat`: lower status of the population (percent) (predictor variable)
13. `medv`: median value of owner-occupied homes in $1000s (predictor variable)
14. `target`: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

## Objective 

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). Below is a short description of the variables of interest in the data set: 

## Dependencies

Replication of our work requires the following packages in Rstudio:

```{r, echo = TRUE, message=FALSE, warning=FALSE, error=FALSE, comment=FALSE}
#install.packages('corrplot')

require(ggplot2)
require(corrplot)
require(dplyr)
require(tidyr)
require(randomForest)
require(forecast)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, comment=FALSE}
# Requirements for formatting and augmenting default settings for chunks. 
require(knitr)
require(kableExtra)
require(default)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

default(kable_styling)  <- list(bootstrap_options = c("basic"), 
                                position = "center", 
                                full_width = TRUE,
                                font_size = NULL)
```

# Data Exploration

First, we read the data as a csv then performed some simple statistical calculations so that we could explore the data. Below we can see a sample of the data output as it was read from the csv.

```{r, echo = FALSE}
training <- as.data.frame(read.csv("crime-training-data_modified.csv"))
test <- as.data.frame(read.csv("crime-evaluation-data_modified.csv"))

training %>% slice(1:5) %>% kable() %>% kable_styling()
```

## Summary Statistics 

We then calculated the mean and standard deviation for each data vector: 

```{r echo=FALSE}
means <- sapply(training, mean)
sds   <- sapply(training, sd)
explore <- as.data.frame(cbind( means, sds))
kable(explore) %>% kable_styling()
```

Below is a bar chart that illutrates the average and standard deviation for each of our data vectors. As we can see, the `tax` vector is a totally different magnitude than the rest. Models involving this vector will benefit from normalization or scaling.

```{r echo=FALSE}
ggplot(explore, aes(x = row.names(explore), y = means))+ 
  geom_bar(stat = 'identity') + 
  labs(title = "Means of Various Features") + 
  xlab("Data Features") + 
  ylab("Mean of Data") +
  theme(panel.background = element_blank()) + 
  geom_errorbar(aes(ymin = means - sds, ymax = means + sds))
```

## Correlation  

We can see our correlation matrix below. A dark blue circle represents a strong positive relationship and a dark red circle represents a strong negative relationship between two variables. We can see that `indus`, `nox`, `target`, and `dis` have the most colinearity. Likewise, these vectors are the best predictors for the target value. Note that this plot only includes rows tha have data in each column.

```{r, echo = FALSE}
results <- cor(training, method = 'pearson', use = 'complete.obs')
corrplot::corrplot(results, method = 'circle')
```
**EDIT QUESTION: SEEING AS THERE ARE NO NA VALUES, CAN WE JUST MENTION THAT FIRST THEN DO ONE CORRPLOT? DOING TWO FOR MISSING ROW VALUES SEEMS REDUNDANT**

We can compare the plot above to the one below, which includes rows without all of the data present. The availability of data does not significantly affect the results.

```{r, echo= FALSE}
results <- cor(training, method = 'pearson')
corrplot::corrplot(results, method = 'circle')
```

We can explore how many `NAs` are in each column to see if we need to impute any of the variables:

```{r echo=FALSE}
apply(training, 2, function(x) length(!is.na(x))) %>% t() %>% kable() %>% kable_styling()
```

As we can see, each data vector has the same number of entries, 466. Imputation will not be necessary. Finally, we can use the `randomforest` package to verify our assumptions from the correlation plot.

```{r, echo = FALSE}
training2 <- training
training2$target <- NULL
target <- training$target

fit <- randomForest(training2, target, importance = TRUE, ntree = 1000)

varImpPlot(fit)
```

We verified our assumptions above using 1000 random forests. The `nox`,  `rad`, `indus`, and `tax` have the most effect. While `dis`is strongly colinear, it has less effect on the target. This is likely due to it encoding information stored redundantly in another vector. 

# Data Preparation

In the exploration section, we identified that there were no missing values in the dataset would affect the outcome of our model. We additionally found that the following four variables had the strongest correlation with our target goal:

1. `nox`
2. `rad` 
3. `indus`
4. `tax`

In the following section, we will analyze and transform these variables to use in the development of our model: 

## Variable Exploration 

### `nox` variable

This variable represents nitrogen oxides concentration (parts per 10 million). The plots below show the `nox` data is multimodal, skewed right, and centered around 0.55. The red line shows the median, whereas the blue line depicts the mean value for this variable. 

```{r echo=F}
ggplot(training, aes(x=nox))+
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  geom_vline(aes(xintercept=median(nox, na.rm=T)),  
               color="red", linetype="dashed", size=1) + 
  geom_vline(aes(xintercept=mean(nox, na.rm=T)),  
               color="blue", linetype="dashed", size=1)
  

summary(training$nox)
```

The `qqplot` below confirms that this variable does not follow a normal distribution. 

```{r echo=F}
qqnorm(training$nox)
qqline(training$nox)
```


```{r}
stud_teach_ratio <- crime_train$ptratio

stud_teach_ratio <- cut(stud_teach_ratio, breaks = 3, labels=c("Small", "Medium", "Large"))
```

## `rad`

This an index variable that represents accessibility to radial highways. The plots below show the `rad` data is bimodal and centered around 5. This data almost follows a normal distribution, however there is an extreme outlier that skews the mean to the right. The red line shows the median, whereas the blue line depicts the mean value for this variable. 

```{r echo=F}
ggplot(training, aes(x=rad))+
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  geom_vline(aes(xintercept=median(rad, na.rm=T)),  
               color="red", linetype="dashed", size=1) + 
  geom_vline(aes(xintercept=mean(rad, na.rm=T)),  
               color="blue", linetype="dashed", size=1)
  

summary(training$rad)
```

The `qqplot` below confirms that this variable does not follow a normal distribution. 

```{r echo=F}
qqnorm(training$rad)
qqline(training$rad)
```

## `indus` 

This variable represents the proportion of non-retail business acres per suburb. The plots below show the `indus` data is bimodal, skewed right, and centered around 10. The red line shows the median, whereas the blue line depicts the mean value for this variable. 

```{r echo=F}
ggplot(training, aes(x=indus))+
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  geom_vline(aes(xintercept=median(indus, na.rm=T)),  
               color="red", linetype="dashed", size=1) + 
  geom_vline(aes(xintercept=mean(indus, na.rm=T)),  
               color="blue", linetype="dashed", size=1)

summary(training$indus)
```

The `qqplot` below confirms that this variable does not follow a normal distribution. 

```{r echo=F}
qqnorm(training$indus)
qqline(training$indus)
```

## `tax` 

This variable represents full-value property-tax rate per $10,000. The values stored in this variable are significantly larger than the ones previously explored. The plots below show that the `tax` data is bimodal, skewed right, and centered around 330. The red line shows the median, whereas the blue line depicts the mean value for this variable. 

```{r echo=F}
ggplot(training, aes(x=tax))+
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  geom_vline(aes(xintercept=median(tax, na.rm=T)),  
               color="red", linetype="dashed", size=1) + 
  geom_vline(aes(xintercept=mean(tax, na.rm=T)),  
               color="blue", linetype="dashed", size=1)


summary(training$tax)
```

The `qqplot` below confirms that this variable does not follow a normal distribution. 

```{r echo=F}
qqnorm(training$tax)
qqline(training$tax)
```

## Variable Transformation 

Our goal is to transform the following data 
```{r}
data <-  training %>% select(nox, rad, indus, tax)
data %>% slice(1:5) %>% kable() %>% kable_styling()
```



We can fix this with a Box-Cox transformation, using the `forecast` package in R. 

```{r}
data2 <- data
transform <- function(x){
  x <- BoxCox(x, lambda = BoxCox.lambda(x))
}
data2 <- as.data.frame(sapply(data, transform))
sapply(data2, hist)
sapply(data2, qqnorm)
```


# Build Models

- [ ] 3 binary logistic models
- [ ] forward, stepwise, random forest, etc
- [ ] Inferences
- [ ] Coefficients

# Select Models

- [ ] Use Log Likelihood, AIC, ROC curve,
- [ ] Evaluate Training Set
- [ ] Accuracy, Error, Precision, Sensitivity, Specificity, F1 score, AUC, conf matrix (hint: use assignment 2, and check out[this link](https://stackoverflow.com/questions/13548266/define-all-functions-in-one-r-file-call-them-from-another-r-file-how-if-pos) )
- [ ] Make predictions with test set and interpret

