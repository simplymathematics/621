---
title: "HW 3"
author: "Team 2"
date: "April 10, 2019"
output:
  pdf_document:
    toc: yes
  html_document:
    pdf_document: default
    theme: cosmo
    toc: yes
    toc_float: yes
---

# Overview

In this homework assignment, you will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). Below is a short description of the variables of interest in the data set: 
 
1. `zn`: proportion of residential land zoned for large lots (over 25000 square feet) (predictor variable)
2. `indus`: proportion of non-retail business acres per suburb (predictor variable)
3. `chas`: a dummy var. for whether the suburb borders the Charles River (1) or not (0) (predictor variable)
4. `nox`: nitrogen oxides concentration (parts per 10 million) (predictor variable)
5. `rm`: average number of rooms per dwelling (predictor variable)
6. `age`: proportion of owner-occupied units built prior to 1940 (predictor variable)
7. `dis`: weighted mean of distances to five Boston employment centers (predictor variable)
8. `rad`: index of accessibility to radial highways (predictor variable)
9. `tax`: full-value property-tax rate per $10,000 (predictor variable)
10. `ptratio`: pupil-teacher ratio by town (predictor variable)
11. `black`: 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town (predictor variable)
12. `lstat`: lower status of the population (percent) (predictor variable)
13. `medv`: median value of owner-occupied homes in $1000s (predictor variable)
14. `target`: whether the crime rate is above the median crime rate (1) or not (0) (response variable)

## Objective 

Your objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. You will provide classifications and probabilities for the evaluation data set using your binary logistic regression model. You can only use the variables given to you (or variables that you derive from the variables provided). 

## Dependencies

Replication of our work requires the following packages in Rstudio:

```{r, echo = TRUE, message=FALSE, warning=FALSE, error=FALSE, comment=FALSE}
#install.packages('corrplot')
#install.packages('randomForest')
#install.packages('olsrr')

require(ggplot2)
require(dplyr)
require(tidyr)
require(corrplot)
require(randomForest)
require(olsrr)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, comment=FALSE}
# Requirements for formatting and augmenting default settings for chunks. 
require(knitr)
require(kableExtra)
require(default)

knitr::opts_chunk$set(echo = T, message = T, warning = T) # change message/warning to F when finished

#default(kable_styling)  <- list(bootstrap_options = c("basic"), 
                               # position = "center", 
                                #full_width = TRUE,
                                #font_size = NULL)
```

# Data Exploration

First, we read the data as a csv then performed some simple statistical calculations so that we could explore the data. Below we can see a sample of the data output as it was read from the csv.

```{r, echo = FALSE}
training <- as.data.frame(read.csv("https://raw.githubusercontent.com/simplymathematics/621/master/HW3/crime-training-data_modified.csv"))
test <- as.data.frame(read.csv("https://raw.githubusercontent.com/simplymathematics/621/master/HW3/crime-evaluation-data_modified.csv"))

training %>% slice(1:5) %>% kable() %>% kable_styling()
```

We can explore how many `NAs` are in each column to see if we need to impute any of the variables:

```{r echo=FALSE}
apply(training, 2, function(x) length(!is.na(x))) %>% t() %>% kable() %>% kable_styling()
```

As we can see, each data vector has the same number of entries, 466. Thus, imputation will not be necessary.

## Summary Statistics 

We then calculated the mean and standard deviation for each data vector: 

```{r echo=FALSE}
means <- sapply(training, mean)
sds   <- sapply(training, sd)
explore <- as.data.frame(cbind( means, sds))
kable(explore) %>% kable_styling()
```

Below is a bar chart that illutrates the average and standard deviation for each of our data vectors. As we can see, the `tax` vector is a totally different magnitude than the rest. Models involving this vector will benefit from normalization or scaling.

```{r echo=FALSE}
ggplot(explore, aes(x = row.names(explore), y = means))+ 
  geom_bar(stat = 'identity') + 
  labs(title = "Means of Various Features") + 
  xlab("Data Features") + 
  ylab("Mean of Data") +
  theme(panel.background = element_blank()) + 
  geom_errorbar(aes(ymin = means - sds, ymax = means + sds))
```

## Histogram 

The following histograms help visualize the spread and skewness of the raw data. 

```{r fig.height=10}
ggplot(data = gather(training), mapping = aes(x = value)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  facet_wrap(~key, ncol = 2, scales = 'free') 
```

## Correlation  

We can see our correlation matrix below. A dark blue circle represents a strong positive relationship and a dark red circle represents a strong negative relationship between two variables. We can see that `indus`, `nox`, `target`, and `dis` have the most colinearity. Likewise, these vectors are the best predictors for the target value. Note that this plot only includes rows tha have data in each column.

```{r, echo = FALSE}
results <- cor(training, method = 'pearson', use = 'complete.obs')
corrplot::corrplot(results, method = 'circle')
```

Finally, we can use the `randomforest` package to verify our assumptions from the correlation plot.

```{r, echo = FALSE}
training2 <- training
training2$target <- NULL
target <- training$target

fit <- randomForest(training2, target, importance = TRUE, ntree = 1000)

varImpPlot(fit)
```

We verified our assumptions above using 1000 random forests. The `nox`,  `rad`, `indus`, and `tax` have the most effect. While `dis`is strongly colinear, it has less effect on the target. This is likely due to it encoding information stored redundantly in another vector. 

# Data Preparation

In the following section, we will prepare and transform our variables for our model: 

## Transformations for Multicollinearity

We saw some correlation between our predictor variables in our exploritory correlation plots. We can test this correlation using variance inflation factors (VIF) to ensure our model is not affected by multicollinearity. 

```{r echo=F}
lm <- lm(target~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+lstat+medv, data=training) 
vif <- ols_vif_tol(lm)
high_mc <- vif %>% filter(VIF > 5) %>% mutate(Standard_Error = sqrt(VIF))
kable(high_mc) %>% kable_styling()
```

This test shows us that the `rad` and `tax` variables have high multicollinearity above 5. Both variables should not be used together, without transformation in our model. The above table shows that as the standard error for both exceeds 2 times the amount then if these variables were not related. 

Rad is an index variable that represents accessibility to radial highways. We choose to bifucate this data using the median value, 5.  

```{r echo=F}
training2<- mutate(training, rad = if_else(rad < median(rad), 0, 1)) 

lm <- lm(target~zn+indus+chas+nox+rm+age+dis+rad+tax+ptratio+lstat+medv, data=training2) 
vif <- ols_vif_tol(lm) %>% mutate(Standard_Error = sqrt(VIF)) %>% t()
kable(vif) %>% kable_styling()
```

```{r}
ggplot(training2, aes(x=rad))+geom_bar()
```

Through this change, the `tax` and `rad` variables are no longer affected by multicollinearity.

## Log Transformations

While logistic modeling does not require normalized data, we choose to apply log transformations to adjust the scales for `age`,  `lstat`, and `tax` so that the variables better fit our models.  

```{r fig.height=10}
training2 <- training2 %>%
   mutate_at(.vars = vars(age, lstat, tax), .funs = log)

training2 %>% select(age, lstat,tax) %>% gather() %>% ggplot(mapping = aes(x = value)) + 
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  facet_wrap(~key, ncol = 2, scales = 'free') 
```

This transformation helps center the `age` and normalize the `lstat` and `tax` variables. 

## New Variables

We additionally chose to create several variables from our initial dataset. 

### `ptratio`

We first changed `ptratio`, a pupil-teacher ratio measurement, into a categorial variable. In the new variable, 0 represents small, 1 represents medium, and 3 represents large ratios. 

```{r echo=F}
training2<- mutate(training2, ptratio = as.numeric(cut(ptratio, breaks = 3, labels=c(0, 1, 3)))) 
```

Our new variable for `ptratio` now looks like this: 

```{r echo=F}
ggplot(training2, aes(x=ptratio))+geom_bar()
```

### `indus` 

This variable represents the proportion of non-retail business acres per suburb. The plots below show the `indus` data is bimodal, skewed right, and centered around 10. The red line shows the median, whereas the blue line depicts the mean value for this variable. 

```{r echo=F}
ggplot(training, aes(x=indus))+
  geom_histogram(aes(y=..density..), colour="black", fill="white")+
  geom_density(alpha=.2, fill="lightgrey")+
  geom_vline(aes(xintercept=median(indus, na.rm=T)),  
               color="red", linetype="dashed", size=1) + 
  geom_vline(aes(xintercept=mean(indus, na.rm=T)),  
               color="blue", linetype="dashed", size=1)
```


We choose to bifuncate this variable using its median value.


```{r echo=F}
training2<- mutate(training2, indus = if_else(indus < median(indus), 0, 1)) 
```

```{r echo=F}
ggplot(training2, aes(x=indus))+geom_bar()
```

As a result of these transformations, our data now looks like this: 

```{r, echo = F}
transformed.data <- as.data.frame(training2)
kable(head(transformed.data)) %>% kable_styling()
```

Build Models

```{r, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, comment=FALSE}
library(forecast)
library(randomForest)
library(e1071)
library(ggplot2)
library(cowplot)
library(MASS)
library(tidyverse)
library(broom)
library(car)
library(caret)
library(pscl)
```

```{r, echo = F}
# training <- as.data.frame(read.csv("https://raw.githubusercontent.com/simplymathematics/621/master/HW3/crime-training-data_modified.csv"))
training$target<-as.factor(training$target)
training$chas<-as.factor(training$chas)
```


Spliting data set on train and test.

```{r, echo = F}
set.seed(123)
training.samples <- training$target %>% 
createDataPartition(p = 0.8, list = FALSE)
train.data  <- training[training.samples, ]
test.data <- training[-training.samples, ]
```

Confusion matrix function.

```{r, echo = F}
matrix<-function(model_name){
        probabilities <- model_name %>% predict(test.data, type = "response")
        predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
        confusionMatrix(as.factor(predicted.classes), as.factor(test.data$target), dnn = c("Prediction", "Reference"))
}

```

MODEL 1 
-------

Basic Model

```{r, echo = F}
model_1<- step(glm(target~., data = training, family = 'binomial'), direction = 'backward')
summary(model_1)
matrix(model_1)
pR2(model_1)
```

Check model_1 for the following logistic regression assumptions:

1. The outcome is a binary (True)
2. There is a linear relationship between the logit of the outcome and each predictor variables (If not, model can benefit from variables transformations)
3. There is no influential values (extreme values or outliers) in the continuous predictors.
4. There is no high intercorrelations (i.e. multicollinearity) among the predictors.

Checking for a linear relationship between the logit of the outcome and each predictor variables

```{r, echo = F}
probabilities <- predict(model_1, type = "response")
predicted.classes <- ifelse(probabilities > 0.5, 1, 0)
# Select only numeric predictors
mydata <- training %>%
  dplyr::select_if(is.numeric) 
predictors <- colnames(mydata)
# Bind the logit and tidying the data for plot
mydata <- mydata %>%
  mutate(logit = log(probabilities/(1-probabilities))) %>%
  gather(key = "predictors", value = "predictor.value", -logit)

ggplot(mydata, aes(logit, predictor.value))+
  geom_point(size = 0.5, alpha = 0.5) +
  geom_smooth(method = "loess") + 
  theme_bw() + 
  facet_wrap(~predictors, scales = "free_y")

```

The relationships are not linear, model can benefit from varibles transformations.


Checking model_1 for the presence of influencial(leverage) values 

```{r, echo = F}
plot(model_1, which = 4, id.n = 5)
# Extract model results
model.data <- augment(model_1) %>% 
  mutate(index = 1:n()) 
model.data %>% top_n(5, .cooksd)
ggplot(model.data, aes(index, .std.resid)) + 
  geom_point(aes(color = target), alpha = .5) +
  theme_bw()
model.data %>% 
  filter(abs(.std.resid) > 3)
```

Eliminating the row with influential value.

```{r, echo = F}
training_clean <-training %>% 
  filter(!(nox==0.464 & age==42.1))

```

Checking for intercorrelations.

```{r, echo = F}
vif(model_1)
```

There is no significant multicollinearity detected in model_1.

MODEL 2 
--------

building a model based on a dateset with eliminated influential values

```{r, echo = F}
model_2<- step(glm(target~., data = training_clean, family = 'binomial'), direction = 'backward')
summary(model_2)
vif(model_2)
matrix(model_2)
pR2(model_2)
```

MODEL 3
--------

Applying BoxCox variables transformations

```{r, echo = F}
model_3 = train(target ~., data = training_clean, 
              method = "glm", family = "binomial",
              preProcess = c("BoxCox"),trControl = trainControl(
                  method = "cv", number = 10,
                  savePredictions = TRUE))

#  variables that were affected by BoxCox
                  
process <- preProcess(dplyr::select(training_clean, -target),
                        method = c("BoxCox"))
process$method
summary(model_3)
vif(model_3$finalModel)


probabilities <- model_3 %>% predict(test.data, type = "prob")
predicted.classes <- ifelse((probabilities[,2]) > 0.5, 1, 0)

confusionMatrix(as.factor(predicted.classes), as.factor(test.data$target), dnn = c("Prediction", "Reference"))

```


MODEL 4 
-------

Based on important variables.

Selecting important variables using caret package

```{r, echo = F}
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(target~., data=training, method="glm",  trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
```


```{r, echo = F}
model_4<- glm(target~., data = training %>% dplyr::select(-lstat, -rm), family = 'binomial')
summary(model_4)
vif(model_4)
matrix(model_4)
pR2(model_4)
```


MODEL 5 
--------

Based on the lowest Akaike information criterion (AIC). MASS package is used.

```{r, echo = F}
# Fit the model
model_5 <- glm(target ~., data = train.data, family = binomial) %>%
  stepAIC(trace = FALSE)
# Summarize the final selected model
summary(model_5)
matrix(model_5)
pR2(model_5)
```


# Select Models

- [ ] Use Log Likelihood, AIC, ROC curve,
Let's compare all 5 models using AIC,  BIC and Log likelihood
```{r}
library(knitr)
library(ggplot2)

library(moments)

library(ggthemes)

library(qqplotr)

library(gridExtra)

library(car)

library(geoR)

library(alr3)

library(rcompanion)

library(sme)

library(caret)

library(pROC)


```



```{r}
m1<-cbind(AIC=AIC(model_1),BIC=BIC(model_1), loglik=logLik(model_1))
m2<-cbind(AIC=AIC(model_2),BIC=BIC(model_2), loglik=logLik(model_2))
#m3<-cbind(AIC=AIC(model_3),BIC=BIC(model_3), loglik=logLik(model_3))
m4<-cbind(AIC=AIC(model_4),BIC=BIC(model_4), loglik=logLik(model_4))
m5<-cbind(AIC=AIC(model_5),BIC=BIC(model_5), loglik=logLik(model_5))
rbind(m1,m2,m4,m5)
```
Considering AIC and BIC model5 has the lowest AIC and BIC which is indicative of a superior fit over all the other models.
Higher log likelihood are associated with better model fits.Using that process might direct to choose a model that is overfitted.
We will choose model 5 as the best model for this assignment.

###Best Model metric
```{r}

training$predict<-predict(model_5, training, type='response')

myRoc <- plot.roc(training$target, training$predict)

myRoc["auc"]
```
The area under the curve is 0.975. The closer we get to 1 the better is the prediction.

Let's choose a cut off probability measure for predicting with a high or low crime rate.
```{r}
#Using the `coords()` function in the pRoc package, the optimal measure is: 
coords(myRoc, "best", ret="threshold",best.method = "youden")
```

The value is closed to 50% let's use 50% as a cutoff.

```{r}
training$predict_target<-ifelse(training$predict>=0.4,1,0)
cm<-confusionMatrix(as.factor(training$predict_target), as.factor(round(training$predict)),positive = "1")

cm$table

#Accuracy
acc<-cm$overall["Accuracy"]; names(acc)<-NULL

#Error Rate
err<-1 - acc; names(err)<-NULL

#Precision
prec<-cm$byClass["Precision"]; names(prec)<-NULL

#Sensivity
sens<-cm$byClass["Sensitivity"]; names(sens)<-NULL

#Specifity
spec<-cm$byClass["Specificity"]; names(spec)<-NULL

#F1 Score
f1<-cm$byClass["F1"]; names(f1)<-NULL

list(accuracy=acc, error_rate=err, precision=prec, sensitivity=sens, specificity=spec, F1=f1)
```

We notice high values of sensitivity and specificity.  Now let's test our data with test set provided.


#Prediction
```{r}
test$predict_prob<-predict(model_5,test, type='response')
test$predict_target<-ifelse(test$predict_prob>=0.4,1,0)
write.csv(test, "prediction.csv", row.names=FALSE)
```




